{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51ea7901-bdb4-434b-b187-cec61a711e68",
   "metadata": {},
   "source": [
    "The code used in this project is inspired from the original project https://github.com/serendipity1122/Pre-trained-Model-Guided-Fine-Tuning-for-Zero-Shot-Adversarial-Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4a4e10-ac95-4a67-b310-5fa1c8a6a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import open_clip\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69cc6393-b524-4e9e-be0e-2a1af3d674b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse the labels from the label file\n",
    "def load_labels(label_file):\n",
    "    labels = []\n",
    "    with open(label_file, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            filename, description = line.strip().split(\"|\")\n",
    "            labels.append((filename, description))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ea3c8b-59e0-410b-b7a5-d91b01f13eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for loading images and their corresponding descriptions\n",
    "class SceneDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, description = self.labels[idx]\n",
    "        image_path = os.path.join(self.image_dir, filename)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da320a6a-40c4-4a59-a654-838f11260691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to load the dataset for Training\n",
    "def prepare_dataset(image_dir, label_file):\n",
    "    # Load labels\n",
    "    labels = load_labels(label_file)\n",
    "    \n",
    "    # Filter images that exist in the directory\n",
    "    filtered_labels = []\n",
    "    for filename, description in labels:\n",
    "        if os.path.exists(os.path.join(image_dir, filename)):\n",
    "            filtered_labels.append((filename, description))\n",
    "        else:\n",
    "            print(f\"Warning: {filename} not found in {image_dir}\")\n",
    "    \n",
    "    # Define transformations for the images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to 224x224 for CLIP\n",
    "        transforms.ToTensor(),         # Convert to PyTorch tensor\n",
    "        transforms.Normalize(mean=[0.481, 0.457, 0.408], std=[0.268, 0.261, 0.275])  # Normalize for CLIP\n",
    "    ])\n",
    "    \n",
    "    # Create the dataset and dataloader\n",
    "    dataset = SceneDataset(image_dir, filtered_labels, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2932f80-e0f5-4c85-bd89-04593e3ab87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"./pmgaft_dataset/images\"\n",
    "label_file = \"./pmgaft_dataset/pmgaft_labels.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34baf7b4-11b6-4554-a827-83ed7007394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataloader = prepare_dataset(image_dir, label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a7aef4-3c2a-4757-a7ca-90f351e93279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCLIP Model and Preprocessor Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the OpenCLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load tokenizer for text input\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "print(\"OpenCLIP Model and Preprocessor Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f6948b8-63ed-44b8-aa01-1244317c4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_embeddings(descriptions, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Generate text embeddings for scene descriptions.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Tokenize descriptions\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    text_embeddings = model.encode_text(text_tokens).detach()  # Detach to freeze embeddings\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "709020a8-fac8-4571-95fc-a8db2b1e5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_embeddings(images, model):\n",
    "    \"\"\"\n",
    "    Generate image embeddings for scene images.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Generate embeddings\n",
    "    image_embeddings = model.encode_image(images).detach()  # Detach to freeze embeddings\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fc27876-e238-4db7-a3b6-21eb9bda4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(image_embeddings, text_embeddings):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between image and text embeddings.\n",
    "    \n",
    "    \"\"\"\n",
    "    similarity = F.cosine_similarity(image_embeddings, text_embeddings)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7bad005-45b3-483b-a4d8-67595fb9a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adversarial_examples(model, images, text_embeddings, epsilon=0.03, alpha=0.01, steps=10):\n",
    "    \"\"\"\n",
    "    Generate adversarial examples using PGD.\n",
    "    \n",
    "    \"\"\"\n",
    "    images_adv = images.clone().detach().requires_grad_(True).to(device)\n",
    "\n",
    "    for step in range(steps):\n",
    "        outputs = model.encode_image(images_adv)\n",
    "        logits = outputs @ text_embeddings.T\n",
    "        loss = F.cross_entropy(logits, torch.arange(images.shape[0], device=device))\n",
    "\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        grad = images_adv.grad.data\n",
    "\n",
    "        # Update adversarial examples\n",
    "        images_adv = images_adv + alpha * grad.sign()\n",
    "        images_adv = torch.clamp(images_adv, images - epsilon, images + epsilon).detach().requires_grad_(True)\n",
    "\n",
    "    return images_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d96bad4-8924-42c6-9e04-879ea5b8a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating the loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79c74d39-ccc3-47e6-b178-7eb0b5b15db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(model, images_adv, images_clean, text_embeddings):\n",
    "    \"\"\"\n",
    "    Compute robustness, generalization, and regularization losses.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Robustness Loss\n",
    "    logits_adv = model.encode_image(images_adv) @ text_embeddings.T\n",
    "    labels = torch.arange(images_adv.shape[0], device=device)\n",
    "    loss_robust = F.cross_entropy(logits_adv, labels)\n",
    "\n",
    "    # Generalization Loss\n",
    "    logits_clean = model.encode_image(images_clean) @ text_embeddings.T\n",
    "    loss_general = F.kl_div(F.log_softmax(logits_adv, dim=-1), F.softmax(logits_clean, dim=-1), reduction=\"batchmean\")\n",
    "\n",
    "    # Regularization Loss\n",
    "    loss_regular = F.mse_loss(logits_adv, logits_clean)\n",
    "\n",
    "    return loss_robust, loss_general, loss_regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69ee3e71-055e-49df-8822-c025ef5a26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.7  # Weight for generalization loss\n",
    "beta = 0.3   # Weight for regularization loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1037e1-8dd0-449c-8c26-e8adc968617a",
   "metadata": {},
   "source": [
    "# Test without Regularization Loss i.e Clean Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18091694-d2e1-4107-a795-98fa7b5a8709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.0722\n",
      "Epoch 2/5, Loss: 1.3165\n",
      "Epoch 3/5, Loss: 0.6404\n",
      "Epoch 4/5, Loss: 1.0622\n",
      "Epoch 5/5, Loss: 0.6503\n",
      "Time taken: 136.26 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for images, descriptions in dataloader:\n",
    "        # Extract text embeddings\n",
    "        text_embeddings = extract_text_embeddings(descriptions, tokenizer, model)\n",
    "\n",
    "        # Generate clean and adversarial image embeddings\n",
    "        images_clean = images.to(device)\n",
    "        images_adv = generate_adversarial_examples(model, images_clean, text_embeddings)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_robust, loss_general, loss_regular = compute_losses(model, images_adv, images_clean, text_embeddings)\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = loss_robust + alpha * loss_general \n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}\")\n",
    "end_time = time.time()\n",
    "time_taken_seconds = end_time - start_time\n",
    "\n",
    "# Convert to minutes\n",
    "time_taken_minutes = time_taken_seconds / 60\n",
    "print(f\"Time taken: {time_taken_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f83d849b-1de5-41fa-a31c-08f7bd6593a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), \"pmgaft_self_driving_finetuned_ablation_study_without_regularization.pth\")\n",
    "print(\"Fine-tuned model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b4be2ab-01b6-43b1-871e-be8dcd77e62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1500 test samples!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load test data from JSON file\n",
    "with open(\"./pmgaft_dataset/test/pmgaft_test_data.json\", \"r\") as json_file:\n",
    "    test_data = json.load(json_file)\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test samples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f287fc4-b4dc-472e-9611-2eeeae654ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_data, image_dir, transform=None):\n",
    "        self.test_data = test_data\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.test_data[idx]\n",
    "        image_path = os.path.join(self.image_dir, entry[\"image\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        descriptions = entry[\"descriptions\"]\n",
    "        ground_truth_index = entry[\"ground_truth_index\"]\n",
    "\n",
    "        return image, descriptions, ground_truth_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54d46ec3-65b0-4a4d-8edd-815e1d58cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.481, 0.457, 0.408], std=[0.268, 0.261, 0.275])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87e5adf7-f2b7-42b8-b6fd-b1816e8bde94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataloader created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the test images\n",
    "image_dir = \"./pmgaft_dataset/images\"\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "test_dataset = TestDataset(test_data, image_dir, transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "print(\"Test dataloader created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8452431-574d-46f3-b8d7-348909fdfe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Clean Images: 0.2807\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, image, descriptions, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict the index of the best matching description.\n",
    "    \"\"\"\n",
    "    # Tokenize and encode descriptions\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    text_embeddings = model.encode_text(text_tokens).detach()\n",
    "\n",
    "    # Encode the image\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    image_embedding = model.encode_image(image)\n",
    "\n",
    "    # Compute similarities and find the best match\n",
    "    similarities = image_embedding @ text_embeddings.T\n",
    "    predicted_index = torch.argmax(similarities, dim=1).item()\n",
    "\n",
    "    return predicted_index\n",
    "\n",
    "# Evaluate accuracy on clean images\n",
    "clean_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Move data to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Get the predicted description index\n",
    "    predicted_index = get_predictions(model, image.squeeze(0), descriptions[0], tokenizer)\n",
    "\n",
    "    # Compare with the ground truth\n",
    "    if predicted_index == ground_truth_index.item():\n",
    "        clean_correct += 1\n",
    "    total += 1\n",
    "\n",
    "clean_accuracy = clean_correct / total\n",
    "print(f\"Accuracy on Clean Images: {clean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e8a9d80-a4fe-492c-a776-de23e1d9a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_for_adversarial(model, adversarial_image, descriptions, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict the index of the best matching description for adversarial images.\n",
    "    \"\"\"\n",
    "    # Ensure descriptions is a list of strings\n",
    "    descriptions = descriptions[0] if isinstance(descriptions, tuple) else descriptions\n",
    "\n",
    "    # Tokenize and encode descriptions\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    text_embeddings = model.encode_text(text_tokens).detach()  # Shape: (num_descriptions, embedding_dim)\n",
    "\n",
    "    # Encode the adversarial image\n",
    "    adversarial_image = adversarial_image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    image_embedding = model.encode_image(adversarial_image)  # Shape: (1, embedding_dim)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = image_embedding @ text_embeddings.T  # Shape: (1, num_descriptions)\n",
    "    predicted_index = torch.argmax(similarities, dim=1).item()  # Get the index of the best match\n",
    "\n",
    "    return predicted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1bb88b2-3e39-46c6-af38-e920948676aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Adversarial Images: 0.6273\n"
     ]
    }
   ],
   "source": [
    "adversarial_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Flatten descriptions to ensure a list of strings\n",
    "    descriptions = [desc[0] if isinstance(desc, tuple) else desc for desc in descriptions]\n",
    "\n",
    "    # Move ground truth index to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Tokenize and encode descriptions\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    text_embeddings = model.encode_text(text_tokens).detach()  # Shape: (num_descriptions, embedding_dim)\n",
    "\n",
    "    # Ensure image is correctly shaped\n",
    "    image_batch = image.to(device)  # No need to unsqueeze as it already has batch dimension [1, 3, 224, 224]\n",
    "\n",
    "    # Generate adversarial examples\n",
    "    images_adv = generate_adversarial_examples(model, image_batch, text_embeddings)  # Shape: [1, 3, 224, 224]\n",
    "\n",
    "    # Get predictions for adversarial images\n",
    "    predicted_index_adv = get_predictions_for_adversarial(model, images_adv.squeeze(0), descriptions, tokenizer)\n",
    "\n",
    "    # Compare predictions with ground truth\n",
    "    if predicted_index_adv == ground_truth_index.item():\n",
    "        adversarial_correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Compute accuracy\n",
    "adversarial_accuracy = adversarial_correct / total\n",
    "print(f\"Accuracy on Adversarial Images: {adversarial_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82690cc9-a0da-449d-87e0-912feddd1a24",
   "metadata": {},
   "source": [
    "# Rest the Model and Test without Robust Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88049d55-c938-4b13-aab7-cd76c115990f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCLIP Model and Preprocessor Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the OpenCLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load tokenizer for text input\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "print(\"OpenCLIP Model and Preprocessor Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7194c04-9c1a-496d-baf8-bb1f815cf97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 14.3461\n",
      "Epoch 2/5, Loss: 11.9035\n",
      "Epoch 3/5, Loss: 15.4699\n",
      "Epoch 4/5, Loss: 17.3418\n",
      "Epoch 5/5, Loss: 14.2555\n",
      "Time taken: 195.64 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for images, descriptions in dataloader:\n",
    "        # Extract text embeddings\n",
    "        text_embeddings = extract_text_embeddings(descriptions, tokenizer, model)\n",
    "\n",
    "        # Generate clean and adversarial image embeddings\n",
    "        images_clean = images.to(device)\n",
    "        images_adv = generate_adversarial_examples(model, images_clean, text_embeddings)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_robust, loss_general, loss_regular = compute_losses(model, images_adv, images_clean, text_embeddings)\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = alpha * loss_general + beta * loss_regular\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}\")\n",
    "end_time = time.time()\n",
    "time_taken_seconds = end_time - start_time\n",
    "\n",
    "# Convert to minutes\n",
    "time_taken_minutes = time_taken_seconds / 60\n",
    "print(f\"Time taken: {time_taken_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d306112-009f-4af0-a364-98e4b1cf2488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), \"pmgaft_self_driving_finetuned_ablation_study_without_robust.pth\")\n",
    "print(\"Fine-tuned model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0aa63861-61fa-41aa-94c0-4519c9abb1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Clean Images: 0.2807\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on clean images\n",
    "clean_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Move data to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Get the predicted description index\n",
    "    predicted_index = get_predictions(model, image.squeeze(0), descriptions[0], tokenizer)\n",
    "\n",
    "    # Compare with the ground truth\n",
    "    if predicted_index == ground_truth_index.item():\n",
    "        clean_correct += 1\n",
    "    total += 1\n",
    "\n",
    "clean_accuracy = clean_correct / total\n",
    "print(f\"Accuracy on Clean Images: {clean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "deefa0d1-149b-46a4-b6e4-c2dfca7b17ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Adversarial Images: 0.4567\n"
     ]
    }
   ],
   "source": [
    "adversarial_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Flatten descriptions to ensure a list of strings\n",
    "    descriptions = [desc[0] if isinstance(desc, tuple) else desc for desc in descriptions]\n",
    "\n",
    "    # Move ground truth index to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Tokenize and encode descriptions\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    text_embeddings = model.encode_text(text_tokens).detach()  # Shape: (num_descriptions, embedding_dim)\n",
    "\n",
    "    # Ensure image is correctly shaped\n",
    "    image_batch = image.to(device)  # No need to unsqueeze as it already has batch dimension [1, 3, 224, 224]\n",
    "\n",
    "    # Generate adversarial examples\n",
    "    images_adv = generate_adversarial_examples(model, image_batch, text_embeddings)  # Shape: [1, 3, 224, 224]\n",
    "\n",
    "    # Get predictions for adversarial images\n",
    "    predicted_index_adv = get_predictions_for_adversarial(model, images_adv.squeeze(0), descriptions, tokenizer)\n",
    "\n",
    "    # Compare predictions with ground truth\n",
    "    if predicted_index_adv == ground_truth_index.item():\n",
    "        adversarial_correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Compute accuracy\n",
    "adversarial_accuracy = adversarial_correct / total\n",
    "print(f\"Accuracy on Adversarial Images: {adversarial_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9159e875-79d6-4003-aed9-bdd5bba2e02c",
   "metadata": {},
   "source": [
    "# Rest the Model and Test without General Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8bf3e758-8dc2-428f-ac38-849663b6d764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCLIP Model and Preprocessor Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the OpenCLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load tokenizer for text input\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "print(\"OpenCLIP Model and Preprocessor Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3df4eaee-d314-499c-8ddd-06058d1a113f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 18.8988\n",
      "Epoch 2/5, Loss: 34.3687\n",
      "Epoch 3/5, Loss: 23.7347\n",
      "Epoch 4/5, Loss: 25.3908\n",
      "Epoch 5/5, Loss: 20.9353\n",
      "Time taken: 185.58 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for images, descriptions in dataloader:\n",
    "        # Extract text embeddings\n",
    "        text_embeddings = extract_text_embeddings(descriptions, tokenizer, model)\n",
    "\n",
    "        # Generate clean and adversarial image embeddings\n",
    "        images_clean = images.to(device)\n",
    "        images_adv = generate_adversarial_examples(model, images_clean, text_embeddings)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_robust, loss_general, loss_regular = compute_losses(model, images_adv, images_clean, text_embeddings)\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = loss_robust + beta * loss_regular\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}\")\n",
    "end_time = time.time()\n",
    "time_taken_seconds = end_time - start_time\n",
    "\n",
    "# Convert to minutes\n",
    "time_taken_minutes = time_taken_seconds / 60\n",
    "print(f\"Time taken: {time_taken_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3db0ec29-3495-4441-9df0-d2e177becc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), \"pmgaft_self_driving_finetuned_ablation_study_without_general.pth\")\n",
    "print(\"Fine-tuned model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73006efa-af24-4f5e-afff-49133039a9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Clean Images: 0.2807\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on clean images\n",
    "clean_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Move data to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Get the predicted description index\n",
    "    predicted_index = get_predictions(model, image.squeeze(0), descriptions[0], tokenizer)\n",
    "\n",
    "    # Compare with the ground truth\n",
    "    if predicted_index == ground_truth_index.item():\n",
    "        clean_correct += 1\n",
    "    total += 1\n",
    "\n",
    "clean_accuracy = clean_correct / total\n",
    "print(f\"Accuracy on Clean Images: {clean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "674de2b6-5d2f-425a-a932-2638d697fbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Adversarial Images: 0.4567\n"
     ]
    }
   ],
   "source": [
    "adversarial_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Flatten descriptions to ensure a list of strings\n",
    "    descriptions = [desc[0] if isinstance(desc, tuple) else desc for desc in descriptions]\n",
    "\n",
    "    # Move ground truth index to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Tokenize and encode descriptions\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    text_embeddings = model.encode_text(text_tokens).detach()  # Shape: (num_descriptions, embedding_dim)\n",
    "\n",
    "    # Ensure image is correctly shaped\n",
    "    image_batch = image.to(device)  # No need to unsqueeze as it already has batch dimension [1, 3, 224, 224]\n",
    "\n",
    "    # Generate adversarial examples\n",
    "    images_adv = generate_adversarial_examples(model, image_batch, text_embeddings)  # Shape: [1, 3, 224, 224]\n",
    "\n",
    "    # Get predictions for adversarial images\n",
    "    predicted_index_adv = get_predictions_for_adversarial(model, images_adv.squeeze(0), descriptions, tokenizer)\n",
    "\n",
    "    # Compare predictions with ground truth\n",
    "    if predicted_index_adv == ground_truth_index.item():\n",
    "        adversarial_correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Compute accuracy\n",
    "adversarial_accuracy = adversarial_correct / total\n",
    "print(f\"Accuracy on Adversarial Images: {adversarial_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6afb706-0dd8-479c-89c4-c697a3b86784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
