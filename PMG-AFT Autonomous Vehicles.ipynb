{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b0ecc5-a60c-456a-b173-f0782961cda3",
   "metadata": {},
   "source": [
    "The code used in this project is inspired from the original project https://github.com/serendipity1122/Pre-trained-Model-Guided-Fine-Tuning-for-Zero-Shot-Adversarial-Robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4a4e10-ac95-4a67-b310-5fa1c8a6a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import open_clip\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69cc6393-b524-4e9e-be0e-2a1af3d674b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse the labels from the label file\n",
    "def load_labels(label_file):\n",
    "    labels = []\n",
    "    with open(label_file, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            filename, description = line.strip().split(\"|\")\n",
    "            labels.append((filename, description))\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ea3c8b-59e0-410b-b7a5-d91b01f13eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class for loading images and their corresponding descriptions\n",
    "class SceneDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, description = self.labels[idx]\n",
    "        image_path = os.path.join(self.image_dir, filename)\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da320a6a-40c4-4a59-a654-838f11260691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to load the dataset for Training\n",
    "def prepare_dataset(image_dir, label_file):\n",
    "    # Load labels\n",
    "    labels = load_labels(label_file)\n",
    "    \n",
    "    # Filter images that exist in the directory\n",
    "    filtered_labels = []\n",
    "    for filename, description in labels:\n",
    "        if os.path.exists(os.path.join(image_dir, filename)):\n",
    "            filtered_labels.append((filename, description))\n",
    "        else:\n",
    "            print(f\"Warning: {filename} not found in {image_dir}\")\n",
    "    \n",
    "    # Define transformations for the images\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to 224x224 for CLIP\n",
    "        transforms.ToTensor(),         # Convert to PyTorch tensor\n",
    "        transforms.Normalize(mean=[0.481, 0.457, 0.408], std=[0.268, 0.261, 0.275])  # Normalize for CLIP\n",
    "    ])\n",
    "    \n",
    "    # Create the dataset and dataloader\n",
    "    dataset = SceneDataset(image_dir, filtered_labels, transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    \n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2932f80-e0f5-4c85-bd89-04593e3ab87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"./pmgaft_dataset/images\"\n",
    "label_file = \"./pmgaft_dataset/pmgaft_labels.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34baf7b4-11b6-4554-a827-83ed7007394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataloader = prepare_dataset(image_dir, label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a7aef4-3c2a-4757-a7ca-90f351e93279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCLIP Model and Preprocessor Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the OpenCLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load tokenizer for text input\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "print(\"OpenCLIP Model and Preprocessor Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f6948b8-63ed-44b8-aa01-1244317c4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_embeddings(descriptions, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Generate text embeddings for scene descriptions.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Tokenize descriptions\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    text_embeddings = model.encode_text(text_tokens).detach()  # Detach to freeze embeddings\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "709020a8-fac8-4571-95fc-a8db2b1e5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_embeddings(images, model):\n",
    "    \"\"\"\n",
    "    Generate image embeddings for scene images.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Generate embeddings\n",
    "    image_embeddings = model.encode_image(images).detach()  # Detach to freeze embeddings\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fc27876-e238-4db7-a3b6-21eb9bda4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(image_embeddings, text_embeddings):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between image and text embeddings.\n",
    "    \n",
    "    \"\"\"\n",
    "    similarity = F.cosine_similarity(image_embeddings, text_embeddings)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7bad005-45b3-483b-a4d8-67595fb9a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_adversarial_examples(model, images, text_embeddings, epsilon=0.03, alpha=0.01, steps=10):\n",
    "    \"\"\"\n",
    "    Generate adversarial examples using PGD.\n",
    "    \n",
    "    \"\"\"\n",
    "    images_adv = images.clone().detach().requires_grad_(True).to(device)\n",
    "\n",
    "    for step in range(steps):\n",
    "        outputs = model.encode_image(images_adv)\n",
    "        logits = outputs @ text_embeddings.T\n",
    "        loss = F.cross_entropy(logits, torch.arange(images.shape[0], device=device))\n",
    "\n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        grad = images_adv.grad.data\n",
    "\n",
    "        # Update adversarial examples\n",
    "        images_adv = images_adv + alpha * grad.sign()\n",
    "        images_adv = torch.clamp(images_adv, images - epsilon, images + epsilon).detach().requires_grad_(True)\n",
    "\n",
    "    return images_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79c74d39-ccc3-47e6-b178-7eb0b5b15db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(model, images_adv, images_clean, text_embeddings):\n",
    "    \"\"\"\n",
    "    Compute robustness, generalization, and regularization losses.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Robustness Loss\n",
    "    logits_adv = model.encode_image(images_adv) @ text_embeddings.T\n",
    "    labels = torch.arange(images_adv.shape[0], device=device)\n",
    "    loss_robust = F.cross_entropy(logits_adv, labels)\n",
    "\n",
    "    # Generalization Loss\n",
    "    logits_clean = model.encode_image(images_clean) @ text_embeddings.T\n",
    "    loss_general = F.kl_div(F.log_softmax(logits_adv, dim=-1), F.softmax(logits_clean, dim=-1), reduction=\"batchmean\")\n",
    "\n",
    "    # Regularization Loss\n",
    "    loss_regular = F.mse_loss(logits_adv, logits_clean)\n",
    "\n",
    "    return loss_robust, loss_general, loss_regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69ee3e71-055e-49df-8822-c025ef5a26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.7  # Weight for generalization loss\n",
    "beta = 0.3   # Weight for regularization loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18091694-d2e1-4107-a795-98fa7b5a8709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.8661\n",
      "Epoch 2/5, Loss: 1.1564\n",
      "Epoch 3/5, Loss: 0.7847\n",
      "Epoch 4/5, Loss: 0.5517\n",
      "Epoch 5/5, Loss: 0.7715\n",
      "Time taken: 137.45 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for images, descriptions in dataloader:\n",
    "        # Extract text embeddings\n",
    "        text_embeddings = extract_text_embeddings(descriptions, tokenizer, model)\n",
    "\n",
    "        # Generate clean and adversarial image embeddings\n",
    "        images_clean = images.to(device)\n",
    "        images_adv = generate_adversarial_examples(model, images_clean, text_embeddings)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_robust, loss_general, loss_regular = compute_losses(model, images_adv, images_clean, text_embeddings)\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = loss_robust + alpha * loss_general + beta * loss_regular\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}\")\n",
    "end_time = time.time()\n",
    "time_taken_seconds = end_time - start_time\n",
    "\n",
    "# Convert to minutes\n",
    "time_taken_minutes = time_taken_seconds / 60\n",
    "print(f\"Time taken: {time_taken_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f83d849b-1de5-41fa-a31c-08f7bd6593a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), \"pmgaft_self_driving_finetuned.pth\")\n",
    "print(\"Fine-tuned model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b4be2ab-01b6-43b1-871e-be8dcd77e62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1500 test samples!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load test data from JSON file\n",
    "with open(\"./pmgaft_dataset/test/pmgaft_test_data.json\", \"r\") as json_file:\n",
    "    test_data = json.load(json_file)\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test samples!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f287fc4-b4dc-472e-9611-2eeeae654ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_data, image_dir, transform=None):\n",
    "        self.test_data = test_data\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.test_data[idx]\n",
    "        image_path = os.path.join(self.image_dir, entry[\"image\"])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        descriptions = entry[\"descriptions\"]\n",
    "        ground_truth_index = entry[\"ground_truth_index\"]\n",
    "\n",
    "        return image, descriptions, ground_truth_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54d46ec3-65b0-4a4d-8edd-815e1d58cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.481, 0.457, 0.408], std=[0.268, 0.261, 0.275])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87e5adf7-f2b7-42b8-b6fd-b1816e8bde94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataloader created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Directory containing the test images\n",
    "image_dir = \"./pmgaft_dataset/images\"\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "test_dataset = TestDataset(test_data, image_dir, transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "print(\"Test dataloader created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8452431-574d-46f3-b8d7-348909fdfe79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Clean Images: 0.2807\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, image, descriptions, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict the index of the best matching description.\n",
    "    \"\"\"\n",
    "    # Tokenize and encode descriptions\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    text_embeddings = model.encode_text(text_tokens).detach()\n",
    "\n",
    "    # Encode the image\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    image_embedding = model.encode_image(image)\n",
    "\n",
    "    # Compute similarities and find the best match\n",
    "    similarities = image_embedding @ text_embeddings.T\n",
    "    predicted_index = torch.argmax(similarities, dim=1).item()\n",
    "\n",
    "    return predicted_index\n",
    "\n",
    "# Evaluate accuracy on clean images\n",
    "clean_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Move data to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Get the predicted description index\n",
    "    predicted_index = get_predictions(model, image.squeeze(0), descriptions[0], tokenizer)\n",
    "\n",
    "    # Compare with the ground truth\n",
    "    if predicted_index == ground_truth_index.item():\n",
    "        clean_correct += 1\n",
    "    total += 1\n",
    "\n",
    "clean_accuracy = clean_correct / total\n",
    "print(f\"Accuracy on Clean Images: {clean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e8a9d80-a4fe-492c-a776-de23e1d9a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_for_adversarial(model, adversarial_image, descriptions, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict the index of the best matching description for adversarial images.\n",
    "    \"\"\"\n",
    "    # Ensure descriptions is a list of strings\n",
    "    descriptions = descriptions[0] if isinstance(descriptions, tuple) else descriptions\n",
    "\n",
    "    # Tokenize and encode descriptions\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    text_embeddings = model.encode_text(text_tokens).detach()  # Shape: (num_descriptions, embedding_dim)\n",
    "\n",
    "    # Encode the adversarial image\n",
    "    adversarial_image = adversarial_image.unsqueeze(0).to(device)  # Add batch dimension\n",
    "    image_embedding = model.encode_image(adversarial_image)  # Shape: (1, embedding_dim)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = image_embedding @ text_embeddings.T  # Shape: (1, num_descriptions)\n",
    "    predicted_index = torch.argmax(similarities, dim=1).item()  # Get the index of the best match\n",
    "\n",
    "    return predicted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1bb88b2-3e39-46c6-af38-e920948676aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Adversarial Images: 0.6113\n"
     ]
    }
   ],
   "source": [
    "adversarial_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Flatten descriptions to ensure a list of strings\n",
    "    descriptions = [desc[0] if isinstance(desc, tuple) else desc for desc in descriptions]\n",
    "\n",
    "    # Move ground truth index to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Tokenize and encode descriptions\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    text_embeddings = model.encode_text(text_tokens).detach()  # Shape: (num_descriptions, embedding_dim)\n",
    "\n",
    "    # Ensure image is correctly shaped\n",
    "    image_batch = image.to(device)  # No need to unsqueeze as it already has batch dimension [1, 3, 224, 224]\n",
    "\n",
    "    # Generate adversarial examples\n",
    "    images_adv = generate_adversarial_examples(model, image_batch, text_embeddings)  # Shape: [1, 3, 224, 224]\n",
    "\n",
    "    # Get predictions for adversarial images\n",
    "    predicted_index_adv = get_predictions_for_adversarial(model, images_adv.squeeze(0), descriptions, tokenizer)\n",
    "\n",
    "    # Compare predictions with ground truth\n",
    "    if predicted_index_adv == ground_truth_index.item():\n",
    "        adversarial_correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Compute accuracy\n",
    "adversarial_accuracy = adversarial_correct / total\n",
    "print(f\"Accuracy on Adversarial Images: {adversarial_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6327ea6e-ede2-404b-a5ad-59f17e4aa8f5",
   "metadata": {},
   "source": [
    "#### Retraining the Model to test the generalization loss and the regularization loss with equal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b1ad6a9-9910-424f-ad52-906969ab60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0  # Weight for generalization loss\n",
    "beta = 1.0   # Weight for regularization loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb072812-9c14-4583-b10e-dd6e8bf32b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCLIP Model and Preprocessor Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the OpenCLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load tokenizer for text input\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "print(\"OpenCLIP Model and Preprocessor Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dea07274-cf75-48c7-84bb-c42dac8d097c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 37.2035\n",
      "Epoch 2/5, Loss: 41.3302\n",
      "Epoch 3/5, Loss: 51.7592\n",
      "Epoch 4/5, Loss: 64.9808\n",
      "Epoch 5/5, Loss: 50.6268\n",
      "Time taken: 138.51 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for images, descriptions in dataloader:\n",
    "        # Extract text embeddings\n",
    "        text_embeddings = extract_text_embeddings(descriptions, tokenizer, model)\n",
    "\n",
    "        # Generate clean and adversarial image embeddings\n",
    "        images_clean = images.to(device)\n",
    "        images_adv = generate_adversarial_examples(model, images_clean, text_embeddings)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_robust, loss_general, loss_regular = compute_losses(model, images_adv, images_clean, text_embeddings)\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = loss_robust + beta * loss_regular\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}\")\n",
    "end_time = time.time()\n",
    "time_taken_seconds = end_time - start_time\n",
    "\n",
    "# Convert to minutes\n",
    "time_taken_minutes = time_taken_seconds / 60\n",
    "print(f\"Time taken: {time_taken_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3852fc96-93fa-44d4-976a-42af6d411e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Clean Images: 0.2807\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on clean images\n",
    "clean_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Move data to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Get the predicted description index\n",
    "    predicted_index = get_predictions(model, image.squeeze(0), descriptions[0], tokenizer)\n",
    "\n",
    "    # Compare with the ground truth\n",
    "    if predicted_index == ground_truth_index.item():\n",
    "        clean_correct += 1\n",
    "    total += 1\n",
    "\n",
    "clean_accuracy = clean_correct / total\n",
    "print(f\"Accuracy on Clean Images: {clean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0430e6e0-0aee-4d4d-87ab-e33f4015fb78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Adversarial Images: 0.4567\n"
     ]
    }
   ],
   "source": [
    "adversarial_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Flatten descriptions to ensure a list of strings\n",
    "    descriptions = [desc[0] if isinstance(desc, tuple) else desc for desc in descriptions]\n",
    "\n",
    "    # Move ground truth index to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Tokenize and encode descriptions\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    text_embeddings = model.encode_text(text_tokens).detach()  # Shape: (num_descriptions, embedding_dim)\n",
    "\n",
    "    # Ensure image is correctly shaped\n",
    "    image_batch = image.to(device)  # No need to unsqueeze as it already has batch dimension [1, 3, 224, 224]\n",
    "\n",
    "    # Generate adversarial examples\n",
    "    images_adv = generate_adversarial_examples(model, image_batch, text_embeddings)  # Shape: [1, 3, 224, 224]\n",
    "\n",
    "    # Get predictions for adversarial images\n",
    "    predicted_index_adv = get_predictions_for_adversarial(model, images_adv.squeeze(0), descriptions, tokenizer)\n",
    "\n",
    "    # Compare predictions with ground truth\n",
    "    if predicted_index_adv == ground_truth_index.item():\n",
    "        adversarial_correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Compute accuracy\n",
    "adversarial_accuracy = adversarial_correct / total\n",
    "print(f\"Accuracy on Adversarial Images: {adversarial_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60268dc4-66ac-43f9-af09-5d5548f0676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "torch.save(model.state_dict(), \"pmgaft_self_driving_finetuned_ab_1.pth\")\n",
    "print(\"Fine-tuned model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bb45621-57d6-4421-80b5-7d860abdf53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # Weight for generalization loss\n",
    "beta = 0.1  # Weight for regularization loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32d0d406-4df6-4eed-ae2c-348d8abbb7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCLIP Model and Preprocessor Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the OpenCLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load tokenizer for text input\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "print(\"OpenCLIP Model and Preprocessor Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be4937b3-015c-49b3-a8c4-25d72c3b0eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 12.0053\n",
      "Epoch 2/5, Loss: 18.5700\n",
      "Epoch 3/5, Loss: 15.7165\n",
      "Epoch 4/5, Loss: 21.9792\n",
      "Epoch 5/5, Loss: 14.1834\n",
      "Time taken: 135.44 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for images, descriptions in dataloader:\n",
    "        # Extract text embeddings\n",
    "        text_embeddings = extract_text_embeddings(descriptions, tokenizer, model)\n",
    "\n",
    "        # Generate clean and adversarial image embeddings\n",
    "        images_clean = images.to(device)\n",
    "        images_adv = generate_adversarial_examples(model, images_clean, text_embeddings)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_robust, loss_general, loss_regular = compute_losses(model, images_adv, images_clean, text_embeddings)\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = loss_robust + beta * loss_regular\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}\")\n",
    "end_time = time.time()\n",
    "time_taken_seconds = end_time - start_time\n",
    "\n",
    "# Convert to minutes\n",
    "time_taken_minutes = time_taken_seconds / 60\n",
    "print(f\"Time taken: {time_taken_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8f84517-5cac-49d5-b584-7adf69e24fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Clean Images: 0.2807\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on clean images\n",
    "clean_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Move data to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Get the predicted description index\n",
    "    predicted_index = get_predictions(model, image.squeeze(0), descriptions[0], tokenizer)\n",
    "\n",
    "    # Compare with the ground truth\n",
    "    if predicted_index == ground_truth_index.item():\n",
    "        clean_correct += 1\n",
    "    total += 1\n",
    "\n",
    "clean_accuracy = clean_correct / total\n",
    "print(f\"Accuracy on Clean Images: {clean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31d501d9-0e1c-4ed9-a831-20b306985d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Adversarial Images: 0.4567\n"
     ]
    }
   ],
   "source": [
    "adversarial_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Flatten descriptions to ensure a list of strings\n",
    "    descriptions = [desc[0] if isinstance(desc, tuple) else desc for desc in descriptions]\n",
    "\n",
    "    # Move ground truth index to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Tokenize and encode descriptions\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    text_embeddings = model.encode_text(text_tokens).detach()  # Shape: (num_descriptions, embedding_dim)\n",
    "\n",
    "    # Ensure image is correctly shaped\n",
    "    image_batch = image.to(device)  # No need to unsqueeze as it already has batch dimension [1, 3, 224, 224]\n",
    "\n",
    "    # Generate adversarial examples\n",
    "    images_adv = generate_adversarial_examples(model, image_batch, text_embeddings)  # Shape: [1, 3, 224, 224]\n",
    "\n",
    "    # Get predictions for adversarial images\n",
    "    predicted_index_adv = get_predictions_for_adversarial(model, images_adv.squeeze(0), descriptions, tokenizer)\n",
    "\n",
    "    # Compare predictions with ground truth\n",
    "    if predicted_index_adv == ground_truth_index.item():\n",
    "        adversarial_correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Compute accuracy\n",
    "adversarial_accuracy = adversarial_correct / total\n",
    "print(f\"Accuracy on Adversarial Images: {adversarial_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d524666-190d-46e9-87bd-79c36933d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.3  # Weight for generalization loss\n",
    "beta = 0.7  # Weight for regularization loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1acdbb9-18d9-454b-966f-3d71a1be38ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCLIP Model and Preprocessor Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the OpenCLIP model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Load tokenizer for text input\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "print(\"OpenCLIP Model and Preprocessor Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6a526d7-cd3b-491a-b284-4fcd9ff30dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 42.4631\n",
      "Epoch 2/5, Loss: 32.4958\n",
      "Epoch 3/5, Loss: 40.5326\n",
      "Epoch 4/5, Loss: 22.3420\n",
      "Epoch 5/5, Loss: 40.9245\n",
      "Time taken: 132.97 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for images, descriptions in dataloader:\n",
    "        # Extract text embeddings\n",
    "        text_embeddings = extract_text_embeddings(descriptions, tokenizer, model)\n",
    "\n",
    "        # Generate clean and adversarial image embeddings\n",
    "        images_clean = images.to(device)\n",
    "        images_adv = generate_adversarial_examples(model, images_clean, text_embeddings)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_robust, loss_general, loss_regular = compute_losses(model, images_adv, images_clean, text_embeddings)\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = loss_robust + beta * loss_regular\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}\")\n",
    "end_time = time.time()\n",
    "time_taken_seconds = end_time - start_time\n",
    "\n",
    "# Convert to minutes\n",
    "time_taken_minutes = time_taken_seconds / 60\n",
    "print(f\"Time taken: {time_taken_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49c881c3-e16e-45c6-9649-a1fc79adc35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Clean Images: 0.2807\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on clean images\n",
    "clean_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Move data to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Get the predicted description index\n",
    "    predicted_index = get_predictions(model, image.squeeze(0), descriptions[0], tokenizer)\n",
    "\n",
    "    # Compare with the ground truth\n",
    "    if predicted_index == ground_truth_index.item():\n",
    "        clean_correct += 1\n",
    "    total += 1\n",
    "\n",
    "clean_accuracy = clean_correct / total\n",
    "print(f\"Accuracy on Clean Images: {clean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c5f6c04-cf6a-40cd-8beb-9affc6284605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Adversarial Images: 0.4567\n"
     ]
    }
   ],
   "source": [
    "adversarial_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Flatten descriptions to ensure a list of strings\n",
    "    descriptions = [desc[0] if isinstance(desc, tuple) else desc for desc in descriptions]\n",
    "\n",
    "    # Move ground truth index to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Tokenize and encode descriptions\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    text_embeddings = model.encode_text(text_tokens).detach()  # Shape: (num_descriptions, embedding_dim)\n",
    "\n",
    "    # Ensure image is correctly shaped\n",
    "    image_batch = image.to(device)  # No need to unsqueeze as it already has batch dimension [1, 3, 224, 224]\n",
    "\n",
    "    # Generate adversarial examples\n",
    "    images_adv = generate_adversarial_examples(model, image_batch, text_embeddings)  # Shape: [1, 3, 224, 224]\n",
    "\n",
    "    # Get predictions for adversarial images\n",
    "    predicted_index_adv = get_predictions_for_adversarial(model, images_adv.squeeze(0), descriptions, tokenizer)\n",
    "\n",
    "    # Compare predictions with ground truth\n",
    "    if predicted_index_adv == ground_truth_index.item():\n",
    "        adversarial_correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Compute accuracy\n",
    "adversarial_accuracy = adversarial_correct / total\n",
    "print(f\"Accuracy on Adversarial Images: {adversarial_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6166a9-8e77-4219-a6ea-1c3092adddf2",
   "metadata": {},
   "source": [
    "## Loss with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac4aaa16-faa6-4eb6-bcde-c379b109587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_losses(model, images_adv, images_clean, text_embeddings):\n",
    "    \"\"\"\n",
    "    Compute robustness, generalization, and regularization losses.\n",
    "    \"\"\"\n",
    "    # Robustness Loss (Cross-Entropy on adversarial examples)\n",
    "    logits_adv = model.encode_image(images_adv) @ text_embeddings.T\n",
    "    labels = torch.arange(images_adv.shape[0], device=device)\n",
    "    loss_robust = F.cross_entropy(logits_adv, labels)\n",
    "\n",
    "    # Generalization Loss (KL Divergence between adversarial and clean outputs)\n",
    "    logits_clean = model.encode_image(images_clean) @ text_embeddings.T\n",
    "    loss_general = F.kl_div(\n",
    "        F.log_softmax(logits_adv, dim=-1),\n",
    "        F.softmax(logits_clean, dim=-1),\n",
    "        reduction=\"batchmean\"\n",
    "    )\n",
    "\n",
    "    # Regularization Loss (Replaced MSE with Cosine Similarity)\n",
    "    loss_regular = 1 - F.cosine_similarity(logits_adv, logits_clean, dim=-1).mean()\n",
    "\n",
    "    return loss_robust, loss_general, loss_regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e1dd1c35-8021-4c2b-9b8b-89bbab83d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5  # Weight for generalization loss\n",
    "beta = 0.5  # Weight for regularization loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e166fcf4-3c35-44f6-a6dc-9e6525cabae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.4457\n",
      "Epoch 2/5, Loss: 1.1075\n",
      "Epoch 3/5, Loss: 1.4279\n",
      "Epoch 4/5, Loss: 1.2815\n",
      "Epoch 5/5, Loss: 0.7323\n",
      "Time taken: 140.03 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    for images, descriptions in dataloader:\n",
    "        # Extract text embeddings\n",
    "        text_embeddings = extract_text_embeddings(descriptions, tokenizer, model)\n",
    "\n",
    "        # Generate clean and adversarial image embeddings\n",
    "        images_clean = images.to(device)\n",
    "        images_adv = generate_adversarial_examples(model, images_clean, text_embeddings)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_robust, loss_general, loss_regular = compute_losses(model, images_adv, images_clean, text_embeddings)\n",
    "\n",
    "        # Combine losses\n",
    "        total_loss = loss_robust + beta * loss_regular\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss.item():.4f}\")\n",
    "end_time = time.time()\n",
    "time_taken_seconds = end_time - start_time\n",
    "\n",
    "# Convert to minutes\n",
    "time_taken_minutes = time_taken_seconds / 60\n",
    "print(f\"Time taken: {time_taken_minutes:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19d13716-ad78-4ff1-b4a2-35da9385d104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Clean Images: 0.2807\n"
     ]
    }
   ],
   "source": [
    "# Evaluate accuracy on clean images\n",
    "clean_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Move data to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Get the predicted description index\n",
    "    predicted_index = get_predictions(model, image.squeeze(0), descriptions[0], tokenizer)\n",
    "\n",
    "    # Compare with the ground truth\n",
    "    if predicted_index == ground_truth_index.item():\n",
    "        clean_correct += 1\n",
    "    total += 1\n",
    "\n",
    "clean_accuracy = clean_correct / total\n",
    "print(f\"Accuracy on Clean Images: {clean_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6544924-2fb8-4937-a2eb-f0c4529b93ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Adversarial Images: 0.6133\n"
     ]
    }
   ],
   "source": [
    "adversarial_correct = 0\n",
    "total = 0\n",
    "\n",
    "for image, descriptions, ground_truth_index in test_dataloader:\n",
    "    # Flatten descriptions to ensure a list of strings\n",
    "    descriptions = [desc[0] if isinstance(desc, tuple) else desc for desc in descriptions]\n",
    "\n",
    "    # Move ground truth index to device\n",
    "    ground_truth_index = ground_truth_index.to(device)\n",
    "\n",
    "    # Tokenize and encode descriptions\n",
    "    text_tokens = tokenizer(descriptions).to(device)\n",
    "    text_embeddings = model.encode_text(text_tokens).detach()  # Shape: (num_descriptions, embedding_dim)\n",
    "\n",
    "    # Ensure image is correctly shaped\n",
    "    image_batch = image.to(device)  # No need to unsqueeze as it already has batch dimension [1, 3, 224, 224]\n",
    "\n",
    "    # Generate adversarial examples\n",
    "    images_adv = generate_adversarial_examples(model, image_batch, text_embeddings)  # Shape: [1, 3, 224, 224]\n",
    "\n",
    "    # Get predictions for adversarial images\n",
    "    predicted_index_adv = get_predictions_for_adversarial(model, images_adv.squeeze(0), descriptions, tokenizer)\n",
    "\n",
    "    # Compare predictions with ground truth\n",
    "    if predicted_index_adv == ground_truth_index.item():\n",
    "        adversarial_correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Compute accuracy\n",
    "adversarial_accuracy = adversarial_correct / total\n",
    "print(f\"Accuracy on Adversarial Images: {adversarial_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a8aa4-bca7-4945-9867-97e4a470c1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
